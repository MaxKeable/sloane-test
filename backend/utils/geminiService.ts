import dotenv from "dotenv";
import { io } from "../index";
import Chat from "../models/chat";
import {
  GoogleGenerativeAI,
  GenerateContentStreamResult,
} from "@google/generative-ai";

dotenv.config();

/**
 * Takes in a prompt, optional base prompt, optional business prompt, and context, and returns a string.
 *
 * @param {string} prompt - The user's input prompt.
 * @param {string} [basePrompt] - The optional base prompt for the assistant.
 * @param {string} [businessPrompt] - The optional business-specific information prompt from the user.
 * @param {any} context - The context for the conversation (chat history).
 * @returns {Promise<string>} - The response text generated by the Google Gemini API.
 */
interface ImageData {
  mimeType: string;
  data: string;
}
const geminiService = async (
  room: string,
  chatId: string,
  prompt: string,
  context: any,
  basePrompt?: string, // Optional parameter
  businessPrompt?: string, // Optional parameter
  pdfText?: string,
  imageData?: ImageData | null,
  memory?: string
) => {
  // Get the chat from the database
  const chat = await Chat.findById(chatId);
  if (!chat) {
    return;
  }

  console.log({ room, chatId, prompt, basePrompt, businessPrompt, context });

  io.on("connection", (socket) => {
    socket.on("joinRoom", (chatId: string) => {
      socket.join(chatId);
      console.log(`User joined room ${chatId}`);
    });
  });

  const genAI = new GoogleGenerativeAI(
    process.env.GOOGLE_GENERATIVE_AI_API_KEY || ""
  );

  // Construct the system instruction based on optional prompts
  let systemPrompt = "";

  if (memory) {
    systemPrompt += `${memory}\n\nIMPORTANT: Always refer back to any relevant facts from user memory to personalize your replies, such as names, preferences, goals, or instructions.`;
  }

  if (basePrompt) {
    systemPrompt += `${basePrompt}\n\n`;
  }

  if (businessPrompt) {
    systemPrompt += `Business Background:\n${businessPrompt}\n\nUse this only to inform your answers when relevant. Do not repeat it unless the user asks.\n\nAlways respond in a conversational, human tone. Keep answers focused on the user's most recent message. Be clear and helpful without rambling. Keep things concise unless the user asks for detail. Avoid repeating your capabilities or the business background unless requested.\n\nAim to create a friendly back-and-forth flow rather than a long monologue.\n\n`;
  }

  if (!basePrompt && !businessPrompt) {
    systemPrompt = "You are a helpful assistant.";
  }

  systemPrompt += `\n${context}\n\nIMPORTANT: Use this conversation history ONLY for context and continuity. Focus your response on the user's current message. Do not summarize or extensively reference past messages unless specifically asked. Keep responses concise, relevant, and conversational.\n\nFORMATTING: Always format your responses using proper Markdown syntax. Use headers (##, ###), bullet points (-), numbered lists (1.), **bold text**, *italic text*, code blocks (\`\`\`), and other Markdown formatting as appropriate to make your responses well-structured and easy to read.\n`;

  if (pdfText) {
    prompt =
      `Document Content: ${pdfText}. If the user ask question from the document, please answer based on the document content. \nDocument End\n` +
      prompt;
  }

  const model = genAI.getGenerativeModel({
    model: "gemini-2.5-pro",
    systemInstruction: systemPrompt.trim(), // Remove any leading or trailing whitespace
  });
  let result: GenerateContentStreamResult;
  try {
    if (imageData) {
      result = await model.generateContentStream({
        contents: [
          {
            role: "user",
            parts: [
              {
                inlineData: {
                  mimeType: imageData.mimeType,
                  data: imageData.data,
                },
              },
              {
                text: prompt,
              },
            ],
          },
        ],
      });
      prompt = "Image Uploaded\n" + prompt;
    } else {
      result = await model.generateContentStream(prompt);
    }

    let response = "";

    // Stream and emit text chunks to the frontend
    for await (const chunk of result.stream) {
      const chunkText = chunk.text();
      console.log({ chunkText });
      // Accumulate the response
      response += chunkText;

      // Emit each chunk to the frontend
      io.to(chatId).emit("openai_response", response);
    }

    // Emit stream end event after all chunks are processed
    setTimeout(async () => {
      try {
        console.log("Closing Stream");
        io.to(chatId).emit("stream_end", response);

        // Save the chat to the database
        chat.messages.push({
          question: prompt,
          answer: response,
        });

        await chat.save();
      } catch (err) {
        console.error("Failed to save chat:", err);
      }
    }, 2500);
  } catch (error) {
    console.error("Error in streaming response:", error);
  }
};

export default geminiService;
